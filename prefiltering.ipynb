{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA PM2 - Dataset pre-filtering\n",
    "\n",
    "This notebooks aims at pre-filtering the [YouNiverse](https://zenodo.org/records/4650046) dataset to only keep gaming related content. In the detail, we will proceed following the next steps.\n",
    "1. Keep only videos which `category` is `Gaming`.\n",
    "2. Keep only channels which have at least one video in the selected list.\n",
    "3. Keep only time-series for the selected channels.\n",
    "4. Keep only comments for the selected videos.\n",
    "\n",
    "We will export each of the resulting datasets in a separate `.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "VIDEOS_PATH = \"data/youniverse/original/yt_metadata_en.jsonl\"\n",
    "CHANNELS_PATH =  \"data/youniverse/original/df_channels_en.tsv\"\n",
    "TIMESERIES_PATH = \"data/youniverse/original/df_timeseries_en.tsv\"\n",
    "COMMENTS_PATH =  \"data/youniverse/original/youtube_comments.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "As a first step, let's simply grasp the first lines of our dataset so as to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>categories</th><th>channel_id</th><th>crawl_date</th><th>description</th><th>dislike_count</th><th>display_id</th><th>duration</th><th>like_count</th><th>tags</th><th>title</th><th>upload_date</th><th>view_count</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>i64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Film &amp; Animation&quot;</td><td>&quot;UCzWrhkg9eK5I8Bm3HfV-unA&quot;</td><td>&quot;2019-10-31 20:19:26.270363&quot;</td><td>&quot;Lego City Police Lego Firetruc…</td><td>1.0</td><td>&quot;SBqSc91Hn9g&quot;</td><td>1159</td><td>8.0</td><td>&quot;lego city,lego police,lego cit…</td><td>&quot;Lego City Police Lego Firetruc…</td><td>&quot;2016-09-28 00:00:00&quot;</td><td>1057.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 12)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ categorie ┆ channel_i ┆ crawl_dat ┆ descripti ┆ … ┆ tags      ┆ title     ┆ upload_da ┆ view_cou │\n",
       "│ s         ┆ d         ┆ e         ┆ on        ┆   ┆ ---       ┆ ---       ┆ te        ┆ nt       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ str       ┆ str       ┆ ---       ┆ ---      │\n",
       "│ str       ┆ str       ┆ str       ┆ str       ┆   ┆           ┆           ┆ str       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ Film &    ┆ UCzWrhkg9 ┆ 2019-10-3 ┆ Lego City ┆ … ┆ lego      ┆ Lego City ┆ 2016-09-2 ┆ 1057.0   │\n",
       "│ Animation ┆ eK5I8Bm3H ┆ 1 20:19:2 ┆ Police    ┆   ┆ city,lego ┆ Police    ┆ 8         ┆          │\n",
       "│           ┆ fV-unA    ┆ 6.270363  ┆ Lego      ┆   ┆ police,le ┆ Lego      ┆ 00:00:00  ┆          │\n",
       "│           ┆           ┆           ┆ Firetruc… ┆   ┆ go cit…   ┆ Firetruc… ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "with open(VIDEOS_PATH, 'r') as file:\n",
    "    data = [json.loads(next(file)) for _ in range(1)]\n",
    "pl.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want to get **gaming videos**, we need to filter out this category from our dataset. At the same time, we filter out the columns that are not relevant for our analysis. According to our understanding of the dataset, we will keep the following columns :\n",
    "- `title` and `tags`, which contain useful information about the video content\n",
    "- `upload_date`, which may be useful to track the link between subjects and periods\n",
    "- `view_count`, `like_count` and `dislike_count`, which are key indicators of the video popularity\n",
    "- `duration`, which may be useful to track trends per video game\n",
    "- `channel_id` and `display_id`, which are useful`to link videos to channels and comments\n",
    "\n",
    "We drop the `description` column, as it is too heavy to handle over that many videos. We also drop the `categories` column, which is no longer relevant, as well as the `crawl_date` column which is not usefult for our study. Finally, we fill missing values for `tags` with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "total_rows = 72924794\n",
    "chunksize = 100000\n",
    "for json_df in tqdm(\n",
    "    pd.read_json(\n",
    "        VIDEOS_PATH, compression=\"infer\", lines=True, chunksize=chunksize\n",
    "    ),\n",
    "    desc=\"Loading data\",\n",
    "    total=total_rows // chunksize,\n",
    "):\n",
    "    json_df = json_df[\n",
    "        [\n",
    "            \"categories\",\n",
    "            \"title\",\n",
    "            \"tags\",\n",
    "            \"upload_date\",\n",
    "            \"view_count\",\n",
    "            \"like_count\",\n",
    "            \"dislike_count\",\n",
    "            \"duration\",\n",
    "            \"channel_id\",\n",
    "            \"display_id\",\n",
    "        ]\n",
    "    ]\n",
    "    json_df = json_df[json_df[\"categories\"] == \"Gaming\"]\n",
    "    dfs.append(json_df)\n",
    "\n",
    "gaming_df = pl.concat(dfs)\n",
    "gaming_df.drop(columns=[\"categories\"], inplace=True)\n",
    "gaming_df.fillna({\"tags\": \"\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    \"categories\",\n",
    "    \"title\",\n",
    "    \"tags\",\n",
    "    \"upload_date\",\n",
    "    \"view_count\",\n",
    "    \"like_count\",\n",
    "    \"dislike_count\",\n",
    "    \"duration\",\n",
    "    \"channel_id\",\n",
    "    \"display_id\",\n",
    "]\n",
    "\n",
    "json_df = pl.read_ndjson(VIDEOS_PATH, columns=columns_to_keep)\n",
    "gaming_df = json_df.filter(pl.col(\"categories\") == \"Gaming\").drop(\"categories\").fill_null(\"tags\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can convert our `pandas` data frame to `vaex`, which is a out-of-core library that allows us to handle huge datasets efficiently, lazy-loading them in memory. We will then save the filtered dataset to a `hdf5` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaming_vdf = vaex.from_pandas(gaming_df)\n",
    "gaming_vdf.export_hdf5('data/yt_gaming_metadata_en.hdf5')\n",
    "\n",
    "## With polars\n",
    "gaming_df.write_csv('data/yt_gaming_metadata_en.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
