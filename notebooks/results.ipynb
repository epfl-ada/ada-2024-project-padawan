{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2\n",
    "> CS-401 - Applied Data Analysis\n",
    "\n",
    "> *Group Padawan - Fall 2024*\n",
    "\n",
    ">*Maxime Ducourau*, *Mehdi Zoghlami*, *LÃ©opold Henry*, *Martin Catheland*, *Jean Perbet*\n",
    "\n",
    "This notebook is an exploratory analysis of the [YouNiverse](https://zenodo.org/records/4650046) dataset, which contains data about **137k** english-speaking [YouTube](https://youtube.com) channels having more than **10k** subscribers, and their **73M** videos, uploaded between **2005** and **2019**. \n",
    "\n",
    "We chose to focus on the **gaming** category, which is one of the most popular on YouTube. We will try to adress the following **research questions**:\n",
    "- What are the most popular games on YouTube ?\n",
    "- Is there a link between real-world gaming events and releases, and the popularity of games on YouTube ?\n",
    "- What are the most linked communities in the gaming category ?\n",
    "\n",
    "Due to the considerable size of the dataset, we pre-filtered the original dataset to only keep the **gaming videos**, their **comments**, the **channels** that have uploaded at least one video in the gaming category and their **time-series**. We also took advantage of it to remove some useless / heavy-weight fields.\n",
    "\n",
    "This pre-filtering is available in the notebook `prefiltering.ipynb`. Below is a summary of the datasets at our disposal, after pre-filtering.\n",
    "\n",
    "| File | Description | Fields |\n",
    "| --- | --- | -- |\n",
    "| `gaming_videos.hdf5` | Videos | `title`, `tags`, `upload_date`, `view_count`, `like_count`, `dislike_count`, `duration`, `channel_id`, `display_id` |\n",
    "| `gaming_comments.tsv` | Comments | `author`, `video_id`|\n",
    "| `gaming_channels.tsv` | Channels | `channel_id`, `channel_name`, `subscribers` |\n",
    "| `gaming_timeseries.tsv` | Channels time-series | `channel_id`, `datetime`, `views`, `delta_views`, `subs`, `delta_subs`, `videos`, `delta_videos` |\n",
    "\n",
    "We'll first examine the datasets to understand our data, its distribution and its stastistical properties, and then we'll perform some initial opersations to try to answer our previously mentioned research questions. Let's first load the libraries and create the constants we will need later.\n",
    "\n",
    "It is important to notice that we will sometimes use the [`vaex`](https://vaex.io) and [`polars`](https://pola.rs) libraries for large datasets throughout this notebook, which are blazingly fast data frames libraries allowing not to load the entire dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frames\n",
    "import pandas as pd\n",
    "import vaex\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# text processing\n",
    "import string\n",
    "\n",
    "# progress tracking\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# youniverse paths\n",
    "VIDEOS_PATH = \"../data/youniverse/filtered/gaming_videos.hdf5\"\n",
    "CHANNELS_PATH =  \"../data/youniverse/original/df_channels_en.tsv\"\n",
    "TIMESERIES_PATH = \"../data/youniverse/original/df_timeseries_en.tsv\"\n",
    "COMMENTS_PATH =  \"../data/youniverse/filtered/gaming_comments.tsv\"\n",
    "\n",
    "# additional paths\n",
    "GAMES_PATH = \"../data/games.csv\"\n",
    "ESPORTS_PATH = \"../data/esports_tournaments.csv\"\n",
    "WORDS_PATH = \"../data/words_alpha.txt\"\n",
    "\n",
    "# random seed\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel metadata\n",
    "\n",
    "Let's first explore the **channels metadata**, and get some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_df = pd.read_csv(CHANNELS_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly check the number of missing values in order to drop them if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of missing values is pretty negligible, so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_df = channels_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some important information to notice. \n",
    "- The **mean number of subscribers** is **~250k**, whereas the **standard deviation** is more than **1M** : this indicates highly varying data, with extreme outliers: this is expected looking at the **minimum** and **maximum** values, which are **10k** and **112M** respectively.\n",
    "- The same applies to the **number of videos** uploaded by the channels, with a **mean** of **~700** and a **standard deviation** of **~4.5k**.\n",
    "\n",
    "Now, let's plot the distribution of the number of subscribers per channel. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(channels_df['subscribers_cc'], bins=100, range=(0, channels_df['subscribers_cc'].quantile(0.95)), color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Number of subscribers')\n",
    "plt.ylabel('Number of channels')\n",
    "plt.title('Distribution of number of subscribers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprsingly, the distribution is **right-skewed**, with a **long tail** of channels having a very high number of subscribers.\n",
    "\n",
    "Let's bin channels depending on their popularity : different **subscriber bins** often **represent different stages** of a channel's development and **reduces variability** within each segment, which can make trends more **statistically significant** and **clearer** to interpret.\n",
    "\n",
    "The ranges will be as follows.\n",
    "- $10000$ - $100000$: **Small** channels, mostly posting videos as a hobby\n",
    "- $100000$ - $1000000$: **Medium** size channels that start reaching mainstream status.\n",
    "- $1000000$ or more: **Large** channels that can reach worlwide popularity, they are quite rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_channel(df: pd.DataFrame, lower_bound: int, upper_bound: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Segments the channels based on the number of subscribers.\n",
    "    \n",
    "    Args:\n",
    "        df: pd.DataFrame - the channels dataframe to segment\n",
    "        lower_bound: int - the lower bound of the segment\n",
    "        upper_bound: float - the upper bound of the segment\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame - the segmented dataframe\n",
    "    \"\"\"\n",
    "    return df[(df[\"subscribers_cc\"] >= lower_bound) & (df[\"subscribers_cc\"] < upper_bound)]\n",
    "\n",
    "sized_channel_dfs = [\n",
    "    segment_channel(channels_df, 10_000, 100_000),\n",
    "    segment_channel(channels_df, 100_000, 1_000_000),\n",
    "    segment_channel(channels_df, 1_000_000, float('inf'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Small\", \"Medium\", \"Large\"]\n",
    "plt.pie(\n",
    "    [len(df) for df in sized_channel_dfs],\n",
    "    labels=labels, \n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    wedgeprops={'edgecolor': 'black'},\n",
    "    colors=['#ff9999', '#66b3ff', '#99ff99'] \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series\n",
    "\n",
    "Now, before going any further, let's examine the **time-series** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df = pd.read_csv(TIMESERIES_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a quick checks shows that there is not a single missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's just check a few statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it makes no sense to study the **mean** and **standard deviation** of the **views**, **subs** and **videos** columns, as they are **cumulative** values. The meaningful insights are the following.\n",
    "- The **mean weekly delta subscribers** and **views** are **~1.3k** and **~381k** respectively, with huge **standard deviations** also indicating extreme outliers.\n",
    "- The **mean weekly delta videos** is **~1.5**, which seems to a reasonable value, but looking at the **maximum** tells us that one channel once posted more than **31k** videos in a week !\n",
    "\n",
    "Now that we have a better understanding of both our channels and timeseries data, let's try to find a correlation between the **number of subscribers** and the **number of views**. We'll divide our analysis between previously determined *small*, *medium* and *large* channels.\n",
    "\n",
    " 1. Firstly, let's merge the **number of subscribers**  with the **total number of views** for each channel, which is the last value of our timeseries for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = timeseries_df.groupby('channel')['views'].last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sized_sub_views_dfs = [\n",
    "    pd.merge(df, views_df, on='channel', how='inner')\n",
    "    for df in sized_channel_dfs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then we'll leverage the `regplot` function from `seaborn` to plot the **number of subscribers** against the **number of views** for each channel, and color the points depending on the channel's size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # Creates a 1x3 grid of plots\n",
    "for i, (df, name) in enumerate(zip(sized_sub_views_dfs, [\"Small\", \"Medium\", \"Large\"])):\n",
    "    sns.regplot(\n",
    "        data=df,\n",
    "        x=\"views\",\n",
    "        y=\"subscribers_cc\",\n",
    "        scatter_kws={\"alpha\": 0.5},\n",
    "        line_kws={\"color\": \"red\"},\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"{name} gaming channels views vs. subscribers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation coefficient between views and subscribers for gaming channels:\")\n",
    "for df, name in zip(sized_sub_views_dfs, [\"Small\", \"Medium\", \"Large\"]):\n",
    "    corr = df[\"views\"].corr(df[\"subscribers_cc\"])\n",
    "    print(f\"{name} channels: {corr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have evidence that the number of views is positively correlated to the number of subscribers. This correlation is weak for small channels, moderate for medium channels and strong for large channels. \n",
    "\n",
    "We interpret this as follows. Small channels tend to produce content that people will watch **regardless of their subscriber count**, only because they are interested, whereas medium and large channels tend to have a more loyal audience that will watch **most of their content** due to their **popularity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video metadata\n",
    "\n",
    "Let's dive into the **videos metadata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = vaex.open(VIDEOS_PATH).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get some insights about our data, starting with the number of gaming videos we have at our disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(videos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than **13.5 million** videos, that's quite a good knowledge base ! Let's get some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see that, **on average**, between may 2005 and october 2019, the **~137k** studied english channels have **~76k views** per video, and their videos last **~24 minutes**. The same pattern applies for the **views** as for the channels **subscribers**, which makes sense: there are extreme outliers which add a lot of vairance to the metrics.\n",
    "\n",
    "Let's try to visualize the distribution of the number of views and the duration of the videos. We'll create a sample of $5\\%$ of the data to avoid runtime issues, while still being representative of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_sample_df = videos_df.sample(frac=0.05, random_state=RANDOM_STATE)\n",
    "print(f'There are {len(videos_sample_df)} videos in the sample dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert back our `vaex` DataFrame to a `pandas.DataFrame` to have more control over visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_sample_df = videos_sample_df.to_pandas_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(videos_sample_df['duration'], bins=150, range=(0, videos_sample_df['duration'].quantile(0.95)), color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Video duration (seconds)')\n",
    "plt.ylabel('Number of videos')\n",
    "plt.title('Distribution of Video Duration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very interesting here to notice the peak at **10min**, which at the period captured by thie dataset, corresponded to the minimum duration of a video that allowed content creators to freely decide the number of ads they could insert, maximizing their videos' **monetization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(videos_sample_df['view_count'], bins=10, range=(0, videos_sample_df['view_count'].quantile(0.95)), color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Views')\n",
    "plt.ylabel('Number of videos')\n",
    "plt.title('Distribution of Views');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the distribution accross time of the **upload date** of our gaming videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_sample_df[\"upload_date\"] = pd.to_datetime(videos_sample_df[\"upload_date\"])\n",
    "videos_sample_df['year_month'] = videos_sample_df['upload_date'].dt.to_period('M')\n",
    "monthly_counts = videos_sample_df['year_month'].value_counts().sort_index()\n",
    "\n",
    "monthly_counts.plot(kind='line')\n",
    "plt.xlim(right=pd.Timestamp('2019-08-31'))\n",
    "plt.xlabel('Upload Date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Upload Dates')\n",
    "plt.xticks(rotation=45);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It follows the general trend of YouTube, with **increasing uploads** over time.\n",
    "\n",
    "#### Most common tags & titles\n",
    "\n",
    "Now, let's dive deeper into our `title` and `tags` field. We noticed that the `tags` field is a list of tags, where they're separated by a comma. What would be super interesting to do is extracting all different tags and see which ones are the most common. Since we start working with the whole dataset again, we take our `vaex` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts = videos_df['tags'].str.lower().str.split(',').value_counts()\n",
    "tag_counts.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting ! We can already see that `minecraft` is tagged on a huge number of videos.\n",
    "\n",
    "Let's apply the same methodology to `title` filed, but his time splitting the words by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counts = videos_df['title'].str.lower().str.split().value_counts()\n",
    "title_counts.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this is less representative. We can try to check some famous games in the `tag_counts` to evaluate their interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of tags containing \"fifa\": {tag_counts[\"fifa\"]}')\n",
    "print(f'Number of tags containing \"minecraft\": {tag_counts[\"minecraft\"]}')\n",
    "print(f'Number of tags containing \"league of legends\": {tag_counts[\"league of legends\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video games dataset\n",
    "\n",
    " Now, we will cross our dataset with another one, containing an almost-comprehensive list of more than **41k** unique commercial video games. It is available [here](https://www.kaggle.com/datasets/matheusfonsecachaves/popular-video-games). So as to keep this study manageable, we will only focus on the first **~1k** most popular games, since we assume they represent the vast majority of YouTube gaming videos.\n",
    " \n",
    "Let's load and filter out the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = pd.read_csv(GAMES_PATH, index_col=0).drop_duplicates(\"Title\").reset_index(drop=True)\n",
    "games_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `Plays` feature to estimate the popularity of the games, and we'll use a cutoff of **2k** games to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 2000\n",
    "\n",
    "games_df[\"Plays_Numeric\"] = games_df[\"Plays\"].apply(lambda x: float(x.replace('k', '').replace('K', '')) * 1000 if \"k\" in x or \"K\" in x else float(x))\n",
    "games_df = games_df.sort_values(by=\"Plays_Numeric\", ascending=False).drop(columns=[\"Plays_Numeric\"])\n",
    "games_df = games_df.iloc[:cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll remove game names that are too short and those that are common english names, as they are likely to be noise. We pick the list of common english names [here](https://github.com/dwyl/english-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORDS_PATH, \"r\") as f:\n",
    "    words = {line.strip() for line in f}\n",
    "\n",
    "games_df = games_df[(games_df[\"Title\"].str.len() > 4) & ~(games_df[\"Title\"].str.lower().isin(words))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some video games are just specific versions of other games, see for instance the *Tetris* example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df[games_df[\"Title\"].str.contains(\"Tetris\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're not interested in specific versions but rather in the game itself, we will remove all games that are a superstring of another game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = games_df[games_df['Title'].apply(lambda x: not any(other in x for other in games_df['Title'] if x != other))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the titles and tags are quite noisy and may contain a lot of irrelevant information as well as different names for a given video game (e.g. *LoL* for *League of Legends*, *gta* for *Grand Theft Auto V*, etc.), we searched for a good and effective way to extract the game names from the `title` and `tags` field. \n",
    "\n",
    "- We first tried to use **TF-IDF** embeddings, but it did not yield satisfactory results are rare words were given too much importance : a title containing the words *mafia* and *minecraft* would be assigned to the game *mafia ii*, even though it is more likely to be about *minecraft*. \n",
    "- We then tried to use **Sentence Transformers** to embed the game names and the titles/tags, and then compute the cosine similarity between them. It did not yield satisfactory results either. \n",
    "\n",
    "- What works the best is in fact quite simple. We lowercase and remove punctuation from all video games names, titles and tags, and then we start by looking if a game name is entirely contained in the video title. If it is, we assign the video to this game. If it is not, we look if a game name is entirely contained in one of the tags. If there is only one game name, we assign the video to this game. Otherwise, if there is no game or several games in the tags, we do not assign the video. This way, we can assign a game to **~50%** of the videos.\n",
    "\n",
    "During **PM3**, we could go further in our analysis by leveraging **LLMs** to acomplish this task for more videos, but this would require a lot of computational power and time. We could also investigate the use of game initials, since it is a commonly used pattern in video tags (e.g. *gta v* for *Grand Theft Auto V*, *lol* for *League of Legends*, *r6* for *Tom Clancy's Rainbow Six Siege*, *cod* for *Call of Duty*, etc.)\n",
    "\n",
    " We'll only provide a **Proof-of-Concept** here, as it is quite slow (~1h) to process the whole dataset. We'll only process the previously devised sample (`pandas_sample_df`) of $5\\%$ of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert to lowercase and remove punctuation from the name.\n",
    "    \n",
    "    Args:\n",
    "        name: str - Name of the game\n",
    "    \n",
    "    Returns:\n",
    "        str - Processed name\n",
    "    \"\"\"\n",
    "    return name.lower().replace(\",\", \" \").translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "videos_sample_df[\"title\"] = videos_sample_df[\"title\"].apply(preprocess_name)\n",
    "videos_sample_df[\"tags\"] = videos_sample_df[\"tags\"].apply(preprocess_name)\n",
    "game_titles = games_df[\"Title\"].apply(preprocess_name).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_game(title: str, tags: str) -> str:\n",
    "    \"\"\"\n",
    "    Map the video to a game based on the title and tags.\n",
    "    \n",
    "    Args:\n",
    "        title: str - The title of the video.\n",
    "        tags: str - List of tags of the video.\n",
    "        \n",
    "    Returns:\n",
    "        str - The name of the game if the video is related to a game, else None.\n",
    "    \"\"\"\n",
    "    for game in game_titles:\n",
    "        if game in title:\n",
    "            return game\n",
    "     \n",
    "    matched_games = []\n",
    "    for game in game_titles:\n",
    "        if game in tags:\n",
    "            matched_games.append(game)\n",
    "    if len(matched_games) == 1:\n",
    "        return matched_games[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_sample_df[\"video_game\"] = videos_sample_df.progress_apply(lambda row: map_to_game(row[\"title\"], row[\"tags\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of classified games over the sample : {100 - (videos_sample_df[\"video_game\"].isna().sum() / len(videos_sample_df) * 100):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost half of our sample was assigned a game ! We can now explore what are the top-15 most popular games in our dataset !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_sample_df[videos_sample_df[\"video_game\"].notna()].value_counts(\"video_game\")[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, YouTube is dominated by *minecraft*, *fortnite*, *call of duty*, *league of legends*, ...\n",
    "\n",
    "## Esports tournaments dataset\n",
    "\n",
    "Let's focus on our second additional dataset, which contains 200 esports tournaments with large prize moneys. It can be found [here](https://www.kaggle.com/datasets/hbakker/esports-200-tournaments).\n",
    " \n",
    "Let's first load and filter out the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esports = pd.read_csv(ESPORTS_PATH)\n",
    "df_esports.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esports = df_esports.drop(columns=['City', 'GameID'])\n",
    "df_esports.loc[df_esports['Country'].isna(), 'Country'] = 'Online'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the columns to the **appropriate formats** : \n",
    "- we drop rows when `StartDate` is **before January 2015** or `EndDate` is **after September 2019** to match the timeseries dataset\n",
    "- we convert the `StartDate` and `EndDate` columns to `datetime` format, and we handle wrong dates\n",
    "- we exclude tournaments lasting **more than 60 days**, as they are more representative of a season than a tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esports['TotalUSDPrize'] = df_esports['TotalUSDPrize'].round().astype(int)\n",
    "df_esports[['StartDate', 'EndDate']] = df_esports[['StartDate', 'EndDate']].apply(pd.to_datetime, format='%m/%d/%y')\n",
    "df_esports = df_esports[(df_esports['StartDate'] > '2015-01-01') & (df_esports['EndDate'] < '2019-09-30')]\n",
    "\n",
    "swap_mask = df_esports['EndDate'] < df_esports['StartDate']\n",
    "df_esports.loc[swap_mask, ['StartDate', 'EndDate']] = df_esports.loc[swap_mask, ['EndDate', 'StartDate']].values\n",
    "\n",
    "df_esports['Duration'] = (df_esports['EndDate'] - df_esports['StartDate']).dt.days\n",
    "df_esports = df_esports[df_esports['Duration'] <= 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column `video_game`, which contains the name of the game associated to the tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esports['video_game'] = ''\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Fortnite', case=False, na=False), 'video_game'] = 'Fortnite'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('RLCS', case=False, na=False), 'video_game'] = 'Rocket League'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('CS:GO|ESL|Esports Championship Series|ELEAGUE|IEM|MLG|PGL|FACEIT', case=False, na=False), 'video_game'] = 'Counter Strike'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('King', case=False, na=False), 'video_game'] = 'Honor of Kings'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Dota|The International|DAC|Major 20|DreamLeague|EPICENTER', case=False, na=False), 'video_game'] = 'Dota 2'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('League of Legends|LoL|Mid-Season Invitational', case=False, na=False), 'video_game'] = 'League of Legends'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Overwatch', case=False, na=False), 'video_game'] = 'Overwatch'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('PUBG', case=False, na=False), 'video_game'] = 'PUBG'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Call of Duty|CoD|CWL', case=False, na=False), 'video_game'] = 'Call of Duty'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Six Invitational', case=False, na=False), 'video_game'] = 'Rainbow Six Siege'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Halo', case=False, na=False), 'video_game'] = 'Halo'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('NBA', case=False, na=False), 'video_game'] = 'NBA 2K'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Smite', case=False, na=False), 'video_game'] = 'Smite'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Shadowverse', case=False, na=False), 'video_game'] = 'Shadowverse'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Hearthstone', case=False, na=False), 'video_game'] = 'Hearthstone'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('StarCraft|WCS', case=False, na=False), 'video_game'] = 'StarCraft'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('HGC|Heroes of the Storm', case=False, na=False), 'video_game'] = 'Heroes of the Storm'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Mythic', case=False, na=False), 'video_game'] = 'Mythic'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('Quake', case=False, na=False), 'video_game'] = 'Quake'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('eRace|F1', case=False, na=False), 'video_game'] = 'Formula 1'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('CrossFire', case=False, na=False), 'video_game'] = 'CrossFire'\n",
    "df_esports.loc[df_esports['TournamentName'].str.contains('AoV', case=False, na=False), 'video_game'] = 'Arena of Valor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column `TournamentDate`, which is the mean date between `StartDate` and `EndDate`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esports['TournamentDate'] = (df_esports['StartDate'] + (df_esports['EndDate'] - df_esports['StartDate']) / 2).dt.date\n",
    "df_esports['TournamentDate'] = pd.to_datetime(df_esports['TournamentDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df_esports.groupby('video_game')['TotalUSDPrize'].mean().round().astype(int).reset_index()\n",
    "grouped_data.rename(columns={'TotalUSDPrize': 'MeanUSDPrize'}, inplace=True)\n",
    "grouped_data = grouped_data.sort_values(by='MeanUSDPrize', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the mean prize money per game. There seems to be huge differences and the causes and consequences of this could be further analyzed in the our final analysis in PM3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(grouped_data['video_game'], grouped_data['MeanUSDPrize'], color='skyblue')\n",
    "plt.xlabel('Video game', fontsize=12)\n",
    "plt.ylabel('Mean Prize Money (USD)', fontsize=12)\n",
    "plt.title('Mean Prize Money by video game', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a subdataset with the $5$ games with the highest mean prize moneys and only $5$ tournaments maximum for these games. We will use it not to plot too many tournaments further in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_games = grouped_data.head(5)\n",
    "df_esports_top = df_esports[df_esports['video_game'].isin(top_games['video_game'])].groupby('video_game').head(5)\n",
    "df_esports_top.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define below various functions that will be useful in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_per_game(game_df: pd.DataFrame, game_name: str, timeseries_df: pd.DataFrame=timeseries_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a dataframe with only the channels of a specific game present in the timeseries data.\n",
    "\n",
    "    Parameters:\n",
    "    game_df: pd.DataFrame - the dataframe with the channels and their respective games\n",
    "    game_name: str - the name of the game\n",
    "    timeseries_df: pd.DataFrame - the dataframe with the timeseries data\n",
    "\n",
    "    Returns:\n",
    "    timeseries_game: pd.DataFrame - the dataframe with the channels of the specific\n",
    "    game present in the timeseries data\n",
    "    \"\"\"\n",
    "\n",
    "    df_game = game_df[\n",
    "        game_df[\"video_game\"].str.contains(game_name, case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    if not isinstance(df_game, pd.DataFrame):\n",
    "        df_game = df_game.to_pandas_df()\n",
    "\n",
    "    df_game = df_game.drop_duplicates(subset=\"channel_id\")\n",
    "\n",
    "    timeseries_game = timeseries_df[\n",
    "        timeseries_df[\"channel_id\"].isin(df_game[\"channel_id\"])\n",
    "    ]\n",
    "\n",
    "    return timeseries_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_per_subs(timeseries_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign a cluster number to each channel based on the number of subscribers.\n",
    "    \n",
    "    Parameters:\n",
    "    timeseries_df: pd.DataFrame - the dataframe with the subscribers of all channels\n",
    "    \n",
    "    Returns:\n",
    "    df_cluster: pd.DataFrame - the dataframe with the cluster number for each channel\n",
    "    \n",
    "    We keep the last 'subs' value for each 'channel_id' \n",
    "    (and not the first, in case the channel would experience major growth) \n",
    "    and assign a cluster from 1 to 5 based on this value.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_cluster = timeseries_df.copy()\n",
    "    df_cluster['Cluster'] = 0\n",
    "\n",
    "    df_last_subs = df_cluster.drop_duplicates(subset='channel_id', keep='last')[['channel_id', 'subs']]\n",
    "    df_cluster = df_cluster.merge(df_last_subs, on='channel_id', suffixes=('', '_last'))\n",
    "\n",
    "    df_cluster.loc[df_cluster['subs_last'] < 20000, 'Cluster'] = 1\n",
    "    \n",
    "    df_cluster.loc[(df_cluster['subs_last'] >= 20000) \n",
    "                   & (df_cluster['subs_last'] < 50000), 'Cluster'] = 2\n",
    "    \n",
    "    df_cluster.loc[(df_cluster['subs_last'] >= 50000) \n",
    "                   & (df_cluster['subs_last'] < 200000), 'Cluster'] = 3\n",
    "    \n",
    "    df_cluster.loc[(df_cluster['subs_last'] >= 200000) \n",
    "                   & (df_cluster['subs_last'] < 800000), 'Cluster'] = 4\n",
    "    \n",
    "    df_cluster.loc[df_cluster['subs_last'] >= 800000, 'Cluster'] = 5\n",
    "\n",
    "    df_cluster = df_cluster.drop(columns='subs_last')\n",
    "\n",
    "    return df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_per_views(timeseries_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign a cluster to each channel based on the last 'views' value.\n",
    "\n",
    "    Parameters:\n",
    "    timeseries_df: pd.DataFrame - the dataframe with the views of all channels\n",
    "\n",
    "    Returns:\n",
    "    df_cluster: pd.DataFrame - the dataframe with the assigned clusters\n",
    "\n",
    "    We keep the last 'views' value for each 'channel_id'\n",
    "    (and not the first, in case the channel would experience major growth)\n",
    "    and assign a cluster from 1 to 5 based on this value.\n",
    "    \"\"\"\n",
    "\n",
    "    df_cluster = timeseries_df.copy()\n",
    "    df_cluster[\"Cluster\"] = 0\n",
    "\n",
    "    df_last_views = df_cluster.drop_duplicates(subset=\"channel_id\", keep=\"last\")[[\"channel_id\", \"views\"]]\n",
    "    df_cluster = df_cluster.merge(df_last_views, on=\"channel_id\", suffixes=(\"\", \"_last\")    )\n",
    "\n",
    "    df_cluster.loc[df_cluster[\"views_last\"] < 200000, \"Cluster\"] = 1\n",
    "\n",
    "    df_cluster.loc[(df_cluster[\"views_last\"] >= 200000) \n",
    "                   & (df_cluster[\"views_last\"] < 1000000),\"Cluster\",] = 2\n",
    "\n",
    "    df_cluster.loc[(df_cluster[\"views_last\"] >= 1000000) \n",
    "                   & (df_cluster[\"views_last\"] < 6000000),\"Cluster\",] = 3\n",
    "\n",
    "    df_cluster.loc[(df_cluster[\"views_last\"] >= 6000000) \n",
    "                   & (df_cluster[\"views_last\"] < 100000000),\"Cluster\",] = 4\n",
    "\n",
    "    df_cluster.loc[df_cluster[\"views_last\"] >= 100000000, \"Cluster\"] = 5\n",
    "\n",
    "    df_cluster = df_cluster.drop(columns=\"views_last\")\n",
    "\n",
    "    return df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_per_date(df: pd.DataFrame, metrics: str='delta_views') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate sum of views or subs of all channels for the same date.\n",
    "\n",
    "    Parameters:\n",
    "    df: pd.DataFrame - the dataframe with the views of all channels\n",
    "\n",
    "    Returns:\n",
    "    df_sum: pd.DataFrame - the dataframe with the sum for each date\n",
    "    \"\"\"\n",
    "\n",
    "    df_sum = df[[\"datetime\", metrics]].copy()\n",
    "    df_sum[\"datetime\"] = pd.to_datetime(df_sum[\"datetime\"])\n",
    "    df_sum[\"datetime\"] = df_sum[\"datetime\"].dt.floor(\"D\")\n",
    "\n",
    "    df_sum = df_sum.dropna(subset=[\"datetime\", metrics])\n",
    "    df_sum[metrics] = df_sum[metrics].round().astype(\"int64\")\n",
    "\n",
    "    df_sum = (df_sum.groupby(\"datetime\")[metrics].sum().reset_index())\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_per_date(df: pd.DataFrame, metrics: str='delta_views') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate mean of views or subs of all channels for the same date.\n",
    "\n",
    "    Parameters:\n",
    "    df: pd.DataFrame - the dataframe with the views of all channels\n",
    "\n",
    "    Returns:\n",
    "    df_mean: pd.DataFrame - the dataframe with the mean for each date\n",
    "    \"\"\"\n",
    "\n",
    "    df_mean = df[[\"datetime\", metrics]].copy()\n",
    "    df_mean[\"datetime\"] = pd.to_datetime(df_mean[\"datetime\"])\n",
    "    df_mean[\"datetime\"] = df_mean[\"datetime\"].dt.floor(\"D\")\n",
    "\n",
    "    df_mean = df_mean.dropna(subset=[\"datetime\", metrics])\n",
    "    df_mean[metrics] = df_mean[metrics].round().astype(\"int64\")\n",
    "\n",
    "    df_mean = (df_mean.groupby(\"datetime\")[metrics].mean().reset_index())\n",
    "    return df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tournament_dates(df_esports: pd.DataFrame, game_name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return the dates of the tournaments for a specific game.\n",
    "\n",
    "    Parameters:\n",
    "    df_esports: pd.DataFrame - the dataframe with the esports tournaments\n",
    "    game_name: str - the name of the game\n",
    "\n",
    "    Returns:\n",
    "    tournament_dates: pd.Series - the dates of the tournaments for the specific game\n",
    "    \"\"\"\n",
    "\n",
    "    game_df = df_esports[df_esports[\"video_game\"] == game_name]\n",
    "    tournament_dates = game_df[\"TournamentDate\"]\n",
    "\n",
    "    return tournament_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(\n",
    "    timeseries_df: pd.DataFrame,\n",
    "    df_esports: pd.DataFrame,\n",
    "    game_name: str,\n",
    "    window_size: int = 6,\n",
    "    stats_per_date: callable = sum_per_date,\n",
    "    metrics: str = 'delta_views',\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the weekly statistics for the channels of a specific game, clustered by the number of views or subscribers (mean or sum).\n",
    "\n",
    "    Parameters:\n",
    "    timeseries_df: pd.DataFrame - the dataframe with the timeseries data\n",
    "    df_esports: pd.DataFrame - the dataframe with the esports tournaments\n",
    "    game_name: str - the name of the game\n",
    "    window_size: int - the size of the window for the rolling average\n",
    "    stats_per_date: callable - the function to apply to the timeseries data per date\n",
    "    *args: any - additional arguments for the stats_per_date function\n",
    "    \"\"\"\n",
    "\n",
    "    num_clusters = len(timeseries_df[\"Cluster\"].unique())\n",
    "    fig, axs = plt.subplots(num_clusters, 1, figsize=(15, 4 * num_clusters))\n",
    "\n",
    "    tournament_dates = get_tournament_dates(df_esports, game_name)\n",
    "\n",
    "    for cluster_num in range(1, num_clusters + 1):\n",
    "        cluster_df = timeseries_df[timeseries_df[\"Cluster\"] == cluster_num]\n",
    "        cluster_df = stats_per_date(cluster_df, metrics=metrics)\n",
    "\n",
    "        cluster_df.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "        # Weekly frequency to get better plotting results\n",
    "        df_weekly = cluster_df.resample(\"W\").mean()\n",
    "\n",
    "        df_weekly[\"rolling_average\"] = (\n",
    "            df_weekly[stats_per_date(timeseries_df, metrics=metrics).columns[1]]\n",
    "            .rolling(window=window_size)\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        axs[cluster_num - 1].plot(\n",
    "            df_weekly.index, df_weekly[\"rolling_average\"], marker=\"o\"\n",
    "        )\n",
    "\n",
    "        # Plot a green vertical line for each tournament associated to the game\n",
    "        for date in tournament_dates:\n",
    "            axs[cluster_num - 1].axvline(\n",
    "                x=date, color=\"green\", linestyle=\"--\", linewidth=1\n",
    "            )\n",
    "\n",
    "        axs[cluster_num - 1].set_title(\n",
    "            f'Weekly {stats_per_date.__name__.replace(\"_per_date\", \"\")} of {stats_per_date(timeseries_df, metrics=metrics).columns[1]} for {game_name} Channels (Cluster {cluster_num})'\n",
    "        )\n",
    "        axs[cluster_num - 1].grid(True)\n",
    "\n",
    "        axs[cluster_num - 1].set_xlabel(\"Date\")\n",
    "        axs[cluster_num - 1].set_ylabel(stats_per_date(timeseries_df, metrics=metrics).columns[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_delta_views_changes_per_clusters(\n",
    "    game_df: pd.DataFrame,\n",
    "    df_esports: pd.DataFrame,\n",
    "    days: int=30,\n",
    "    cluster_type: callable = cluster_per_subs,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the mean delta views before and after tournaments for each cluster of channels.\n",
    "\n",
    "    Parameters:\n",
    "    game_df: pd.DataFrame - the dataframe with the timeseries data for a specific game\n",
    "    df_esports: pd.DataFrame - the dataframe with the esports tournaments\n",
    "    days: int - the number of days before and after the tournament to consider\n",
    "    cluster_type: callable - the function to assign clusters to channels based on views or subscribers\n",
    "\n",
    "    Returns:\n",
    "    df_differences: pd.DataFrame - the dataframe with the mean delta views before and after tournaments\n",
    "    \"\"\"\n",
    "    \n",
    "    differences = []\n",
    "    unique_games = df_esports[\"video_game\"].unique()\n",
    "\n",
    "    for game in unique_games:\n",
    "\n",
    "        timeseries_game = timeseries_per_game(game_df, game)\n",
    "        timeseries_game = cluster_per_subs(timeseries_game)\n",
    "\n",
    "        tournament_dates = df_esports[df_esports[\"video_game\"] == game][\"TournamentDate\"]\n",
    "\n",
    "        for date in tournament_dates:\n",
    "            date = pd.to_datetime(date)\n",
    "            change = {\"video_game\": game, \"TournamentDate\": date}\n",
    "\n",
    "            for cluster in sorted(timeseries_game[\"Cluster\"].unique()):\n",
    "                cluster_data = timeseries_game[timeseries_game[\"Cluster\"] == cluster]\n",
    "\n",
    "                before_tournament = (\n",
    "                    cluster_data[\"datetime\"] >= date - pd.Timedelta(days=days)\n",
    "                ) & (cluster_data[\"datetime\"] < date)\n",
    "                mean_delta_views_before = cluster_data.loc[\n",
    "                    before_tournament, \"delta_views\"\n",
    "                ].mean()\n",
    "\n",
    "                after_tournament = (cluster_data[\"datetime\"] > date) & (\n",
    "                    cluster_data[\"datetime\"] <= date + pd.Timedelta(days=days)\n",
    "                )\n",
    "                mean_delta_views_after = cluster_data.loc[\n",
    "                    after_tournament, \"delta_views\"\n",
    "                ].mean()\n",
    "\n",
    "                if mean_delta_views_before:\n",
    "                    change_percent = (\n",
    "                        (mean_delta_views_after - mean_delta_views_before)\n",
    "                        / abs(mean_delta_views_before)\n",
    "                    ) * 100\n",
    "                else:\n",
    "                    # Null value when no data or division by zero\n",
    "                    change_percent = None\n",
    "\n",
    "                change[f\"Change_Cluster{cluster} (%)\"] = np.round(change_percent, 2)\n",
    "\n",
    "            differences.append(change)\n",
    "\n",
    "    df_differences = pd.DataFrame(differences)\n",
    "    return df_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dota = timeseries_per_game(games_df, 'Dota 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dota = cluster_per_subs(timeseries_dota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dota.drop_duplicates(subset='channel_id')['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe how the channels are distributed across the five clusters. The clustering appears effective. As it varies for each game, it is expected to not have a 20% distribution across clusters. Instead, we may see more variation, particularly in the middle clusters (2 to 4). Additionally, the number of channels shown is limited as we are currently presenting only a sample. A more comprehensive analysis will be conducted for PM3, using more material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tournament_dates(df_esports_top, 'Dota 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the highest-paid tournament for Dota 2 occurs around the same period each year. In our full analysis, it could be interesting to investigate whether this event drives higher engagement or, conversely, stabilizes the trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(cluster_dota, df_esports_top, 'Dota 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_changes = mean_delta_views_changes_per_clusters(game_df, df_esports, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_changes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_changes = df_changes.dropna(subset=['Change_Cluster1 (%)', \n",
    "                                                      'Change_Cluster2 (%)', \n",
    "                                                      'Change_Cluster3 (%)', \n",
    "                                                      'Change_Cluster4 (%)', \n",
    "                                                      'Change_Cluster5 (%)'], how='all')\n",
    "df_changes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop rows only when all values are null, which likely indicates that there is no time series data available for the tournament date. However, we retain the row if there is at least one non-null value, as this suggests the presence of some time series data, even if there may not be enough data to observe changes across all clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_changes = (df[['Change_Cluster1 (%)', \n",
    "                        'Change_Cluster2 (%)', \n",
    "                        'Change_Cluster3 (%)', \n",
    "                        'Change_Cluster4 (%)', \n",
    "                        'Change_Cluster5 (%)']]>0).all(axis=1).sum()\n",
    "positive_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more than 10% of the rows, the mean of delta_views during the month following a tournament is greater than the month before. Even if this number is not huge, it is still important. Indeed, considering the huge number big video games' followers, this represents an important increase in views in Youtube statistics. Moreover, as already said, it is only a sample so we will likely discover more interesting insights in our final analysis in PM3 and try to uncover patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communities on YouTube\n",
    "\n",
    "We now want to extract community insights using the **comments** part of the dataset. More precisely, we want to link viewers that have commented on **plenty channels**, to see if some patterns emerge.\n",
    "\n",
    "Due to the fact that we have a tremendous amount of data to process, and since we only focus on **Gaming** videos, we start by filtering the dataset to only keep the Gaming comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_df = pl.read_csv(CHANNELS_PATH, separator=\"\\t\")\n",
    "videos_df = pl.read_ndjson(VIDEOS_PATH)\n",
    "\n",
    "channels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to join the data contained in the Channel Data (containing the mapping from the `channel_id` to the `channel_name`), the Video Metadata (containing the mapping from the `video_id` to the `channel_id`), and the Comments (containing the mapping from the `video_id` to the `author`).\n",
    "By doing so we will have the mapping from the `author` to the `channel_name` which will allow us to see the ones that have commented on multiple channels, and observe what are the **most common links** between channels, to identify potential **communities** and clusters.\n",
    "\n",
    "Firstly, we join our Channel Data with the Video Metadata, and keep only the needed columns (`display_id` to map to the comments' authors, `channel` to adress channels by their name instead of ID's, and `subscribers` to represent bigger channels with bigger points on the graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = videos_df.join(channels_df, left_on=\"channel_id\", right_on=\"channel\")\n",
    "\n",
    "joined_df = joined_df.select(\n",
    "    [\n",
    "        pl.col(\"display_id\"),\n",
    "        pl.col(\"channel_id\"),\n",
    "        pl.col(\"name_cc\"),\n",
    "        pl.col(\"subscribers_cc\"),\n",
    "    ]\n",
    ").rename({\"name_cc\": \"channel\", \"subscribers_cc\": \"subscribers\"})\n",
    "\n",
    "joined_df.unique(\"channel\").sort(\"subscribers\").reverse().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to proceed on comments. For **PM2**, we just took a 100M subset of the comments for the sake of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pl.scan_csv(COMMENTS_PATH, separator=\"\\t\", has_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join the previously joined_data with the comments (`display_id` to `video_id`), and we suppress the number of likes and replies since they are not needed for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_video_channels = comments_df.join(\n",
    "    joined_df.lazy(), left_on=\"video_id\", right_on=\"display_id\"\n",
    ")\n",
    "\n",
    "comments_channels = comments_video_channels.select(\n",
    "    [pl.col(\"author\"), pl.col(\"channel\")]\n",
    ").unique()\n",
    "\n",
    "comments_channels.head().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our graph, the users that commented on only one channel are **not relevant** for our analysis, because they don't illustrate any sort of connection between channels communities. We thus filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_author = (\n",
    "    comments_channels.group_by(\"author\")\n",
    "    .agg(pl.col(\"channel\").count().alias(\"count\"))\n",
    "    .filter(pl.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "# We remove the authors that commented on only one channel\n",
    "comments_several_channels = comments_channels.join(\n",
    "    counts_per_author, on=\"author\", how=\"inner\"\n",
    ").select([\"author\", \"channel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final graph input needs to be of the form `source, target, weight`, where `source` and `target` are the channel names, and `weight` is the number of users that commented on both channels. We then aggregate the data to have the final graph.\n",
    "\n",
    "Note that we **alphabetically sorted** the two channels that are linked by the edge, to avoid duplicates (i.e. if an author made comments under channels A, B and C, we would add 3 edges A-B, A-C and B-C but not the B-A, C-A and C-B ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = (\n",
    "    comments_several_channels.join(comments_several_channels, on=\"author\", how=\"inner\")\n",
    "    .filter(pl.col(\"channel\") < pl.col(\"channel_right\"))\n",
    "    .select([\"channel\", \"channel_right\"])\n",
    ")\n",
    "\n",
    "pairs_grouped = pairs.group_by([\"channel\", \"channel_right\"]).agg(\n",
    "    pl.col(\"channel\").count().alias(\"weight\")\n",
    ")\n",
    "\n",
    "pairs_grouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the generation for 100M comments took a lot of time, we decided to **save** the final graph in a `.csv` file to avoid reprocessing the data.\n",
    "\n",
    "To improve the meaning of our edges that connects two channels, we decided to **keep only** the edges that have a weight of **at least 1000**. This way, we can focus on the **most relevant** connections between channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_grouped_filtered = (\n",
    "    pl.scan_csv(\"edges.csv\")\n",
    "    .filter(pl.col(\"weight\") > 1000)\n",
    "    .sort(\"weight\", descending=True)\n",
    ")\n",
    "\n",
    "edges = pairs_grouped_filtered.collect()\n",
    "\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the edges we want, we can get all the **nodes** (channes) from the edges and before plotting the graph, we first get back the **subscribers** count for each channel to represent them with bigger points on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_filtered = edges.select([\"channel\"]).unique()\n",
    "\n",
    "nodes = (\n",
    "    channels_filtered.join(\n",
    "        channels_df, left_on=\"channel\", right_on=\"name_cc\", how=\"inner\"\n",
    "    )\n",
    "    .select([\"channel\", \"subscribers_cc\"])\n",
    "    .sort(\"subscribers_cc\", descending=True)\n",
    ")\n",
    "\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final graph, we used the `networkx` library to plot the graph, and we used the `kamada_kawai_layount`, which separates well the clusters to have a better visualization of the communities. The `best_partition` function from the `community` module allows us to identify the communities in the graph.\n",
    "\n",
    "More precisely, each node represents a **channel** (the size of the node is proportional to the number of subscribers of the channel), and each edge represents a **connection between two channels** (when the weight of the edge is at least 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import community.community_louvain as community\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for row in nodes.iter_rows(named=True):\n",
    "    G.add_node(row[\"channel\"], subscribers=row[\"subscribers_cc\"])\n",
    "\n",
    "for row in edges.iter_rows(named=True):\n",
    "    G.add_edge(row[\"channel\"], row[\"channel_right\"], weight=row[\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community.best_partition(G, weight=\"weight\")\n",
    "pos = nx.kamada_kawai_layout(G, weight=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are defined the **parameters** of the graph, such as **colors and sizes** of different nodes. We defined communities' colors **manually** to have a better visualization of the graph.\n",
    "\n",
    "We also showed on the graph the names of channels that have more than **10M subscribers**, to see where the most famous YouTubers are located in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = set(partition.values())\n",
    "colors = [\n",
    "    \"#39C5DB\",\n",
    "    \"#1AA5E0\",\n",
    "    \"#FF5450\",\n",
    "    \"#FEBA33\",\n",
    "    \"#E08300\",\n",
    "    \"#4FC43D\",\n",
    "    \"#8A52F5\",\n",
    "    \"#3BCEAC\",\n",
    "    \"#F4D35E\",\n",
    "    \"#EE964B\",\n",
    "    \"#8BC34B\",\n",
    "]\n",
    "\n",
    "color_map = dict(zip(clusters, colors))\n",
    "\n",
    "default_color = \"#D3D3D3\"\n",
    "\n",
    "node_colors = [\n",
    "    color_map.get(partition[node], default_color)\n",
    "    for node in G.nodes\n",
    "    if node in partition\n",
    "]\n",
    "\n",
    "edge_colors = [\n",
    "    color_map.get(partition[edge[0]], default_color)\n",
    "    for edge in G.edges\n",
    "    if edge[0] in partition\n",
    "]\n",
    "\n",
    "node_sizes = [G.nodes[node].get('subscribers') for node in G.nodes()]\n",
    "node_sizes = [max(size / 100000, 10) if size else 10 for size in node_sizes]\n",
    "\n",
    "label_threshold = 10000000\n",
    "labels = {node: node if G.nodes[node].get('subscribers') and (G.nodes[node].get('subscribers') > label_threshold) else '' for node in G.nodes()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step is to show the graph and observe it the **communities connections** that emerged from the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx_nodes(\n",
    "    G,\n",
    "    pos,\n",
    "    node_color=node_colors,\n",
    "    node_size=node_sizes,\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=0.1,\n",
    ")\n",
    "\n",
    "nx.draw_networkx_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    labels=labels,\n",
    "    font_size=16,\n",
    "    verticalalignment=\"bottom\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_colors, alpha=0.1)\n",
    "\n",
    "plt.gcf().set_size_inches(40, 30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada-pm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
